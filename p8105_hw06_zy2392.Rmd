---
title: "p8105_hw06_zy2392"
author: "Stephen Yuan"
date: "12/4/2021"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
library(glmnet)
```

## Problem 1

### Import data and tidy data

```{r}
bw_raw = 
  read.csv("birthweight.csv") %>% 
  janitor::clean_names() %>% 
  drop_na() %>% 
  mutate(
    babysex = factor(babysex, levels = c('1','2')),
    frace = factor(frace, levels = c('1','2','3','4','5','6','7','8','9')),
    mrace = factor(mrace, levels = c('1', '2', '3', '4', '8')),
    malform = factor(malform, levels = c('0', '1'))
  )
```

### Model fitting

#### Model selection

Use the **stepwise regression method** to select the best model.

```{r}
model_fit = lm(bwt ~ ., data = bw_raw) %>% 
  step(direction = 'backward')
summary(model_fit)
```

**Modeling process:** The model selection process started with all variables, variables whose loss bring the most statistically insignificant effect on the model will be deleted,
the process will repeat until no further variables deleted could be deleted.

#### Residual plots

```{r}
bw_raw %>% 
  add_predictions(model_fit) %>% 
  add_residuals(model_fit) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = .4, size = 2) +
  xlab("Fitted Values") +
  ylab("Residuals")
```

**Comment:** residuals are centered around 0, however, there is limited amount of outliers.

#### Two other models

main effects only

```{r}
model_1 = lm(bwt ~ blength + gaweeks, data = bw_raw)

model_1 %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
```

including three-way interactions

```{r}
model_2 = lm(bwt ~ bhead * blength * babysex, data = bw_raw)

model_2 %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
```

make comparisons using cross validation

```{r}
cv_df = 
  crossv_mc(bw_raw, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
    ) %>% 
  mutate(model1  = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x)),
         model2  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         model3  = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x))) %>% 
  mutate(rmse_model1 = map2_dbl(model1, test, ~rmse(model = .x, data = .y)),
         rmse_model2 = map2_dbl(model2, test, ~rmse(model = .x, data = .y)),
         rmse_model3 = map2_dbl(model3, test, ~rmse(model = .x, data = .y)))
```

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

We can see from the plots, model 1 had the lowest root-mean-square error and model 2 had the largest root-mean-square error. 
Thus, model 1 had the best performance whereas model 2 had the worst performance, among all three models.

## Problem 2

### Import dataset

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

```{r}
boot_sample = function(df) {
  
  sample_frac(df, replace = TRUE)
  
}
```

### Analyzing R squared.

```{r}
boot_straps = 
  data_frame(
    strap_number = 1:5000,
    strap_sample = rerun(5000, boot_sample(weather_df))
  )
bootstrap_results_1 = 
  boot_straps %>% 
  mutate(
    models = map(strap_sample, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap_sample, -models) %>% 
  unnest(results) 
bootstrap_results_1 %>%
  ggplot(aes(x = adj.r.squared)) + 
  geom_density()
```

From the plot of adjusted R square, we can see that its distribution looks like normal distribution, with a peak between 0.90 and 0.92.

The 95% CI of adjusted R square is: (`r quantile(bootstrap_results_1$adj.r.squared, probs=0.025)` , `r quantile(bootstrap_results_1$adj.r.squared, probs=0.975)`).


### Analyzing log of the product of betas.

```{r}
log_beta_p = function(df) {
    log(df[1,2]*df[2,2]) %>% 
    tibble() %>% 
    mutate(
      log_betas=.$estimate) %>% 
    select(log_betas)
}
```

```{r}
bootstrap_results_2=
  boot_straps %>% 
  mutate(
    models = map(strap_sample, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy),
    log_betas = map(results, log_beta_p)) %>% 
  select(-strap_sample, -models) %>%
  unnest(log_betas)
```

```{r}
bootstrap_results_2 %>%
  ggplot(aes(x = log_betas)) + 
  geom_density()
```

From the plot of log of the product of betas, we can see that its distribution also looks like normal distribution, with a peak between 2.00 and 2.025.

The 95% CI of log of the product of betas is: (`r quantile(bootstrap_results_2$log_betas, probs=0.025)` , `r quantile(bootstrap_results_2$log_betas, probs=0.975)`).
